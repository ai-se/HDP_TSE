

\input{dims}

\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}

\section{Explaining These Results}

The above results are suprising (to say the least): a small numher
of examples is sufficient to build a defect predictor, even when the data is being transferred from columns with other names.
Hence it is important to ask:
\bi
\item Are the above results relevant to just the data used in this study?
\item Or some more fundamental process at work?
\ei
This section argues the latter; i.e. that the above results are not some quirk of our scripts. Rather, it is a result
of core inductive processes.
Specifically:
\bi
\item
If data mining is an instance of sampling theory,
\item Then it is possible to derive
an expression defining claasses of data sets where it is (and is not) possible to  construct
defect predictors using a small number of samples (say 50 to 200).
\ei
The important feature of this model is the
following: there is much evidence that the data used
in defect prediction research belong to the ``a few
samples is enough'' class.  That is, we can expect that our
results are externally valid for defect prediction data.

\subsection{``Chessboard'' Sampling}

Suppose a chessboard is divided into a grid of $b^2$ cells (in standard chess, $b=8$ so the board has 64 cells). Suppose further that the chessbooard
contains some signal we want to detect, but that signal is not spread evenly around the board.
That is to say, some cells of the chessboard are blank which other cells are $e$\% covered
with that signal. The probability of finding that signal is a product of:
\bi
\item The probability $c$ of picking a particular cell;
\item The probability $p$ that, once we arrive at that cell, we will find  the  signal in that cell.
\ei
With a few changes, this chessboard model can be
used to represent the process of machine
learning. For example,
instead of a board with two
dimensions, data mining works on a ``chessboard''
with $d$ dimensions: one for every independent
variable collected from a project (for example, for a list of the
$d=21$ dimensions used in our MORPH data sets of this study,
see \fig{ck}).

Further,
instead of each dimension being divided into eight (like a chessboard), it is standard practice in data mining for SE~\cite{Menzies2014a}
to divide dimensions accroding to some {\em descritization policy}~\cite{lust08}.
Discretization converts a numeric variable with infinite range into a smaller number of  $b$ bins. Hence, the number of cells in a
hyperdimensional chessboard is $b^d$ and the probability of selecting any one cell is
\begin{equation}\label{eq:c}c=1/(d^d)=b^{-d}\end{equation}
Once we arrive at any cells, we will be in a region weith $e$ percent errors.
What are is the probability $p$ that we will find those $e$ errors, given $n$ samples from the training data?
According to Voas and Miller~\cite{voas1995software},
if we see something at probability $e$, then we will miss it at probability $1-e$.
After $n$ attempts, the probability of missing it is $(1-e)^n$ so the probability of stumbling onto $e$ errors is
\begin{equation}\label{eq:p}
p(e,n) = (1-(1-e)^n)
\end{equation}
The premise of data mining is that in the data ``chessboard'', some cells contain more of the signal than others. Hence, the 
distribution of the $e$ errors are ``skewed'' by some factor $k$. If $k=1$, then all the errors are evenly distributed over all cells.
But at all other values of $k$, some cells contain more errors than others, computed as follows:
    \bi
  \item $R_c$ is a random number $0\le R \le 1$, selected for each part of the space $c\in C$.
  \item $x_c$ is the proportion of errors in each part of $C$. \mbox{$x_c =  R_{c\in C}^k$}.
  \item We normalize $x_c$ to be some ratio $0 \le x_c \le 1$ as follows: $X= \sum_{c\in C} x_c$ then $x_c = x_c/X$
    \ei
    If  $e$ is the ratio of classes within a software projects containing errors, then $e'$ 
    the expected value of the probability $p$ of selecting a cell {\em and} that cell containing errors is:
    \begin{equation}\label{eq:p}
      e' = \sum_{c\in C}cx_ce
    \end{equation}
    where $c$ comes from \eq{c} and $e$ is the ratio of classes in the training set with defects.

Using these equations, we can determine how many training examples $n$ are required before 
  $p(e',n)$ (\eq{p}) returns a value more than some reasonable threshold $T$.
  To make that determination, we call $p(e',n)$ for increasing values of $n$ until $p \ge T$
  (for this paper, we used $T \ge 67\%$.
  
    This above maths lets us define
    a  Monte Carlo simulation to assess the external validity of our results.
    1000 times, we picked $k,b,d,e$ values at random from:
    \bi
      \item $k \in {1,2,3,4,5}$;
  \item $d \in {3,4,5,6,7}$ dimensions;
  \item $b \in \{2,3,4,5,6,7\}$ bins; 
    \item $e\in \{0.1,0.2,0.3,0.4\}$
      \ei
      (These ranges were set using our experience with data mining, For example, our prior work shows in defect prediction data sets
      with 40 or more diminesions, that good predictors can be build using $d\le 7$ of those dimensions~\cite{me07a}.)
      
    For each of the 1000 repeats,
     we increased $n$ until \eq{pb} reported that $q$ passed some reasonable threshold (for these experiments, we used $q>67\%$).
     Next, we logged the 1000 examples of   $k,b,d,e,n$ found via this process.
    To help simplify the reporting of these results,
     we divided $n$ into $n<50, n<100, n<200,n\ge200$ which we denoted {\em lt50, lt100, lt200,ge200}, respectively.
    This data was given to a decision tree learner to determine what $n$ values are selected by different
    ranges of $\{k,b,d,e\}$. Decision tree learners seek an attribute range that, when used to split the data,
      simplifies the distribution of the dependent variable in each split (in our case, {\em lt50, lt100, lt200,ge200}).
      The decision tree learner is then called recursively on each split.
      To test the stability of the learned model, the learning is repeated ten times, each time using 90\%of the data from training and the rest
      for testing. The weighted average performance values for the learned decision trees were remarkably good:
      \bi
    \item False alarm rates = 2\%;
    \item F-measures (i.e. the harmonic mean of recall and precision) of 95\%
      \ei
The resulting decision tree, shown in \fig{tree}
shows the external validity of these results. In that tree:
\bi
\item
The green branches show where $n \le 200$ samples would
be enough to detect defective modules at probability
67\% or more.
\item
The red branches where more than $n$ samples would be enough.
\ei
It is useful to consider where defect prediction data sets
fall into that tree.
Numerous
recent research results show that, for defect data,
good predictors can be built via a {\em median chop}
of dimensions~\cite{Zhang2014,nam2015clami};
i.e. $b=2$. Other results show that defect
prediction data containing dozens of attributes also
contains many correlated attributes, and that
feature subset
selection tools can reduce that set to
$d \in \{2,3\}$ dimensions~\cite{Menzies07}.


The top half of that tree shows that when the data
can be reduced to two dimensions, then if each
dimensions needs less than three bins, then {\em
ltt50, lt100, lt200 } samples will suffice (i.e.$n<
200$).  The bottom of tree shows that when the data
needs more than two or three dimensions, and those
dimensions needs only two bins, then no outcome
needs {\em ge200} samples (i.e. $n\ge200$).

The green branches are interesting since 



% some bracksh shown in white
%leaves need to be labelled A,B,C (little black letter on right hand side)
      
