

%\input{dims}

\newcommand{\tion}[1]{Section~\ref{sect:#1}}
% \newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
% \newcommand{\bi}{\begin{itemize}}
% \newcommand{\ei}{\end{itemize}}

\section{Explaining Results of Size Limits for Effective Transfer Learning}\label{sect:xplain}
To assess the generality of the results in Section~\ref{sec:sizelimit}, we need some background knowledge that knows when a few samples will (or will not)
be sufficient to build a defect predictor. Using some sampling theory, this section:
\squishlist
\item
Builds such a mathematical model;
\item Maps known aspects
of defect data  into that model;
\item Identifies what need to change before the above results no longer hold.
\squishend
To start, we repeat the {\em lessons learned} above as well as what is {\em known about defect datasets}.
Next, we define a {\em maths model} which will be used in a {\em Monte Carlo simulation} to generate a log of how many samples are required
to find some signal. This log will be summarized via a {\em decision tree learner}.

\subsection{Set up}
\subsubsection{ Lessons from the above work}

The results in Section~\ref{sec:sizelimit} show
that a small number
of examples are sufficient to build a defect predictor, even when the data is  transferred from columns with other names.
In the following we will build a model to compute the probability that $n$ training examples are sufficient to detect $e$\% defective instances.

In order to simplify the analysis, we divide $n$ into
  $n<50, n<100, n<200$, and $n\ge 200$ four ranges respectively (and note that $n  \ge 200$ is where the above
  results do not hold).

\subsubsection{Known about defect datasets}\label{sect:data}

Recent
results~\cite{Zhang14,nam2015clami} show that, for defect data, good
predictors can be built via a {\em median chop} of
numeric project data;
they are divided into $b=2$ bins, i.e., defective bin and non-defective bin. For example, defective instances that likely have high metric values belong to the defective bin while non-defective ones that have low metric values belong to the non-defective bin~\cite{nam2015clami}.
%\lin{I don't quite understand this. Expand the explanation?}.

Other results~\cite{shepperd94}
show that defect prediction data containing dozens
of attributes, many of which are correlated
attributes. Hence, while a dataset may have many dimensions, it only really ``needs'' a few
(and by ``need'' we mean that adding unnecessary dimensions does not add the accuracy
of defect predictors learned from this data).


Feature subset selection algorithms~\cite{Hall03} can  determine
which  dimensions are needed, and which can be ignored.
When applied to defect data~\cite{Menzies07},
we found that those datasets may only need  $d \in \{2,3\}$
dimensions.

Hence, in the following, we will pay particular attention to the ``typical'' region of
 $b=2, d \le 3$.

\subsubsection{A Mathematical Model}

Before writing down some maths, it is useful to start with some intuitions.
Accordingly, consider a chess board  containing small  piles of defects in some cells.
Like all chess boards, this one is  divided into a grid of $b^2$ cells (in standard chess, $b=8$ so the board has 64 cells).
Further, some cells of the chess board are blank while other cells are $e$\% covered
with that signal.

If we throw a small pebble at that chess board, then  the odds
of hitting a defect is $c \times p$ where:
\squishlist
\item $c$ is the probability of picking a particular cell;
\item $p$ is the probability that, once we arrive at that cell, we will find  the  signal in that cell.
\squishend
With a few changes, this chess board model can be
used to represent the process of machine
learning. For example,
instead of a board with two
dimensions, data mining works on a ``chess board''
with $d$ dimensions: i.e. one for all the independent
variables collected from a project (which are ``needed'', as defined as \tion{data}).

Also,
instead of each dimension being divided into eight (like a chess board), it is common in data mining for SE~\cite{Menzies2014a}
to divide dimensions accroding to some {\em descritization policy}~\cite{lust08}.
Discretization converts a numeric variable with infinite range into a smaller number of  $b$ bins. Hence, the number of cells in a
hyper-dimensional chess board is $b^d$ and the probability of selecting any one cell is
\begin{equation}\label{eq:c}c=1/(b^d)=b^{-d}\end{equation}
Once we arrive at any cells, we will be in a region with $e$ percent errors.
What  is the probability $p$ that we will find those $e$ errors, given $n$ samples from the training data?
According to Voas and Miller~\cite{voas1995software},
if we see something at probability $e$, then we will miss it at probability $1-e$.
After $n$ attempts, the probability of missing it is $(1-e)^n$ so the probability of stumbling onto $e$ errors is:
\begin{equation}\label{eq:p}
p(e,n) = 1-(1-e)^n
\end{equation}
The premise of data mining is that in the data ``chess board'', some cells contain more of the signal than others. Hence, the 
distribution of the $e$ errors are ``skewed'' by some factor $k$. If $k=1$, then all the errors are evenly distributed over all cells.
But at all other values of $k$, some cells contain more errors than others, computed as follows:
    \squishlist
  \item $R_c$ is a random number $0\le R \le 1$, selected for each part of the space $c\in C$.
  \item $x_c$ is the proportion of errors in each part of $C$. \mbox{$x_c =  R_{c\in C}^k$}.
  \item We normalize $x_c$ to be some ratio $0 \le x_c \le 1$ as follows: $X= \sum_{c\in C} x_c$ then $x_c = x_c/X$
    \squishend
    If  $e$ is the ratio of classes within a software project containing errors, then $E$ 
    is the expected value  of selecting a cell {\em and} that cell containing errors:
    \begin{equation}\label{eq:E}
     E   = \sum_{c\in C}c \times x_ce
    \end{equation}
    where $c$ comes from \eq{c} and $e$ is the ratio of classes in the training set with defects.


Using these equations, we can determine how many
training examples $n$ are required before $p(E,n)$, from 
\eq{p}, returns a value more than some reasonable
threshold $T$.  To make that determination, we call
$p(E,n)$ for increasing values of $n$ until $p \ge
T$ (for this paper, we used $T = 67\%$).

For completeness, it should be added  that the procedure of the above paragraph is an {\em upper bound} on the
number of examples needed to find a signal since it
assumes random sampling of a skewed distribution. In
practice, if a data mining algorithm is smart, then
it would {\em increase} the probability of finding
the target signal, thus {\em decreasing} how many samples are required.

\subsubsection{Monte Carlo Simulation}
    The above maths let  us define
    a  Monte Carlo simulation to assess the external validity of our results.
    Within 1000 times of iterations, we picked $k,d,b,e$ values at random from:
    \squishlist
      \item $k \in \{1,2,3,4,5\}$;
  \item $d \in \{3,4,5,6,7\}$ dimensions;
  \item $b \in \{2,3,4,5,6,7\}$ bins; 
    \item $e\in \{0.1,0.2,0.3,0.4\}$
      \squishend
      (These ranges were set using our experience with data mining, For example, our prior work shows in defect prediction datasets
      with 40 or more dimensions, that good predictors can be built using $d\le 7$ of those dimensions~\cite{Menzies07}.)
      
     Within 1000 iterations of Monte Carlo simulation,
     we increased $n$ until \eq{p} showed  $p$ passed our reasonable threshold.
     Next, we generated examples of what $n$ value was found using  $k,b,d,e$.
     
\begin{figure}
\renewcommand{\baselinestretch}{0.75}
\begin{alltt}\scriptsize 
     1	  dimensions = (1,2)
     2	  |   dimensions = 1
     3	  |   |   e \(\le\) 0.1
     4	  |   |   |   bins = (1,2,3) : \(n < 50\) 
     5	  |   |   |   bins > 3       : \(n < 100\) 
     6	  |   |   e > 0.1 : \(n < 50\)  
     7	  |   dimensions > 1
     8	  |   |   bins = (1,2,3)
     9	  |   |   |   e = 0.1 : \(n < 100\) 
    10	  |   |   |   e > 0.1 : \(n < 50\)  
    11	  |   |   bins > 3
    12	  |   |   |   bins = (4,5)
    13	  |   |   |   |   e = 0.1 : \(n < 200\) 
    14	  |   |   |   |   e > 0.1
    15	  |   |   |   |   |   e \(\le\) 0.2
    16	  |   |   |   |   |   |   bins = 4 : \(n < 100\) 
    17	  |   |   |   |   |   |   bins = 5 : \(n < 200\) 
    18	  |   |   |   |   |   e > 0.2      : \(n < 100\) 
    19	  |   |   |   bins > 5
    20	  |   |   |   |   e \(\le\) 0.2 : \(n \ge 200 \) 
    21	  |   |   |   |   e > 0.2 : \(n < 200\)
    22	  dimensions > 2
    23	  |   bins = (1,2)
    24	  |   |   dimensions = (3,4,5)
    25	  |   |   |   dimensions = 3
    26	  |   |   |   |   e \(\le\) 0.2 : \(n < 200\) 
    27	  |   |   |   |   e > 0.2 : \(n < 50\) 
    28	  |   |   |   dimensions = (4,5)
    29	  |   |   |   |   e = 0.1 : \(n < 200\) 
    30	  |   |   |   |   e > 0.1
    31	  |   |   |   |   |   dimensions = 4 : \(n < 100\) 
    32	  |   |   |   |   |   dimensions = 5
    33	  |   |   |   |   |   |   e \(\le\) 0.3 : \(n < 200\) 
    34	  |   |   |   |   |   |   e > 0.3 : \(n < 100\) 
    35	  |   |   dimensions > 5 : \(n \ge 200 \) 
    36	  |   bins > 2
    37	  |   |   dimensions = 3
    38	  |   |   |   bins = (3,4)
    39	  |   |   |   |   e \(\le\) 0.3
    40	  |   |   |   |   |   bins = 3
    41	  |   |   |   |   |   |   e = 0.1 : \(n \ge 200 \) 
    42	  |   |   |   |   |   |   e > 0.1 : \(n < 200\) 
    43	  |   |   |   |   |   bins = 4 : \(n \ge 200 \) 
    44	  |   |   |   |   e > 0.3 : \(n < 200\) 
    45	  |   |   |   bins > 4 : \(n \ge 200 \) 
    46	  |   |   dimensions > 3 : \(n \ge 200 \)
\end{alltt}
\caption{How many $n$ examples are required to be at least 67\%
likely to find defects occurring at probability $e$.}\label{fig:dtree}
\end{figure}
\subsubsection{Decision Tree Learning}

    These examples were given to a decision tree learner to determine what $n$ values are selected by different
    ranges of $\{k,b,d,e\}$. Decision tree learners seek an attribute range that, when used to split the data,
      simplifies the distribution of the dependent variable in each split.
      The decision tree learner is then called recursively on each split.  
      To test the stability of the learned model, the learning is repeated ten times, each time using 90\% of the data from training and the rest
      for testing. The weighted average performance values for the learned decision tree were remarkably good:
\squishlist
    \item False alarm rates = 2\%;
    \item F-measures (i.e. the harmonic mean of recall and precision) of 95\%
\squishend

\subsection{Results}

The resulting decision tree, shown in \fig{dtree}, defined regions where
building defect predictors would be very easy and much harder.
Such trees can be read as nested if-then-else statements. For example, Line 1 is an ``if'',
lines 2 to 21 are the associated ``then'' and the tree starting at Line 22 is the ``else''.
For another example, we could summarise lines 1 to 5 as follows:
\begin{quote}
if there are one dimension and the probability of the defects is less than 10\% then (if
the number of bins per dimension is three or less then 50 samples will suffice; else, up to 100
samples may be required.)
\end{quote}

\squishlist
\item
Lines 2 to 6 discuss a very easy case. Here, we only need
one dimension to build defect predictors and,
for such simple datasets, 50 to 100 examples are enough for defect prediction.
\item
Lines 22, 36, 46 show a branch of the decision tree
where we need many dimensions that divide into many bins.
For such datasets, we require a larger number of samples to learn a predictor ($n \ge 200$).
\squishend
The key part of \fig{dtree} is the ``typical'' region defined in \tion{data};
i.e.    $b=2, d \le 3$:
\squishlist
\item Lines 7 to 10 show one set of branches covering this ``typical'' region. Note
lines 9, 10:  we need up to 100 examples when the defect signal is rare (10\%) but
far fewer when the signal occurs at $e>10$\%.
\item
Lines 22 to 27 show another set of branches in this ``typical region''. Note lines 26, 27:
we need up 50 to 200 examples.
\squishend
\subsection{Summary}
Our experiments with transfer learning showed that 50 to 200 examples are needed for adequate
transfer of defect knowledge. If the reader doubts that this number is too small to be effective,
we note that:
\squishlist
\item Other researchers working with Logistic Regression~\cite{peduzzi1996simulation}
have reported that 10 ``events per decision variable'' are adequate for building models using that
data miner. For our defect datasets, in \fig{small_epv}, we repeated the analysis of~\cite{peduzzi1996simulation}
and found that considering 2 independent variable on average, there was little improvement after 100 examples with EPV = 10 (as would have been predicted by~\cite{peduzzi1996simulation}).
\item The maths of \tion{xplain} show that this ``100 examples are enough'' is a feature of the kinds of
data being explored.
\squishend
