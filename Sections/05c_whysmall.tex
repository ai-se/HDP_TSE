

%\input{dims}

\newcommand{\tion}[1]{\S\ref{sect:#1}}
% \newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
% \newcommand{\bi}{\begin{itemize}}
% \newcommand{\ei}{\end{itemize}}

\section{Explaining Results of Size Limits for Effective Transfer Learning}\label{sect:xplain}
To assess the generality of the above results in Section~\ref{sec:sizelimit}, we need some background knowledge that knows when a few samples will (or will not)
be sufficient to build a defect predictor. Using some sampling theory, this section:
\bi
\item
Builds such a mathematical model;
\item Maps known aspects
of defect data  into that model;
\item Identifies what would need to change before the above results would no longer hold.
\ei
To start, we repeat the {\em lessons learned} above as well as what is {\em known about defect data sets}.
Next, we define a {\em maths model} which will be used in a {\em Monte Carlo simulation} to generate a log of how many samples are required
to find some signal. This log will be summarized via a {\em decision tree learner}.

\subsection{Set up}
\subsubsection{ Lessons from the above work}

The above results show
that a small number
of examples are sufficient to build a defect predictor, even when the data is being transferred from columns with other names.
In the following we will build a model computing the probability that $n$ training examples are sufficient to detect $e$\% defective modules.

In order to simplify the analysis, we will divide $n$ into
  $n<50, n<100, n<200,n\ge 200$ respectively (and note that $n  \ge 200$ is where the above
  results do not hold).

\subsubsection{Known about defect data sets}\label{sect:data}

Recent
results~\cite{Zhang2014,nam2015clami} show that, for defect data, good
predictors can be built via a {\em median chop} of
numeric project data;
i.e. they are divided into $b=2$ bins.

Other results~\cite{shepperd94}
show that defect prediction data containing dozens
of attributes, many of which are correlated
attributes. Hence, while a data set may have many dimensions, it only really ``needs'' a few
(and by ``need'' we mean that adding in unnecessary dimensions does not add to the accuracy
of defect predictors learned from this data).


Feature subset selection algorithms~\cite{Hall03} can  determine
which  dimensions are needed, and which can be ignored.
When applied to defect data~\cite{Menzies07},
we found that those data sets may only need  $d \in \{2,3\}$
dimensions.

Hence, in the following, we will pay particular attention to the ``typical'' region of
 $b=2, d \le 3$.

\subsubsection{A Maths Model}

Before writing down some maths, it is useful to start with some intuitions.
Accordingly, consider a chess board  containing small  piles of defects in some cells.
Like all chess boards, this one is  divided into a grid of $b^2$ cells (in standard chess, $b=8$ so the board has 64 cells).
Further, some cells of the chess board are blank while other cells are $e$\% covered
with that signal.

If we throw a small pebble at that chess board, then  the odds
of hitting a defects is $c \times p$ where:
\bi
\item $c$ is the probability of picking a particular cell;
\item $p$ is the probability that, once we arrive at that cell, we will find  the  signal in that cell.
\ei
With a few changes, this chess board model can be
used to represent the process of machine
learning. For example,
instead of a board with two
dimensions, data mining works on a ``chess board''
with $d$ dimensions: i.e. one for all the independent
variables collected from a project (which are ``needed'', as defined as \tion{data}).

Also,
instead of each dimension being divided into eight (like a chess board), it is common in data mining for SE~\cite{Menzies2014a}
to divide dimensions accroding to some {\em descritization policy}~\cite{lust08}.
Discretization converts a numeric variable with infinite range into a smaller number of  $b$ bins. Hence, the number of cells in a
hyper-dimensional chess board is $b^d$ and the probability of selecting any one cell is
\begin{equation}\label{eq:c}c=1/(b^d)=b^{-d}\end{equation}
Once we arrive at any cells, we will be in a region with $e$ percent errors.
What  is the probability $p$ that we will find those $e$ errors, given $n$ samples from the training data?
According to Voas and Miller~\cite{voas1995software},
if we see something at probability $e$, then we will miss it at probability $1-e$.
After $n$ attempts, the probability of missing it is $(1-e)^n$ so the probability of stumbling onto $e$ errors is:
\begin{equation}\label{eq:p}
p(e,n) = 1-(1-e)^n
\end{equation}
The premise of data mining is that in the data ``chess board'', some cells contain more of the signal than others. Hence, the 
distribution of the $e$ errors are ``skewed'' by some factor $k$. If $k=1$, then all the errors are evenly distributed over all cells.
But at all other values of $k$, some cells contain more errors than others, computed as follows:
    \bi
  \item $R_c$ is a random number $0\le R \le 1$, selected for each part of the space $c\in C$.
  \item $x_c$ is the proportion of errors in each part of $C$. \mbox{$x_c =  R_{c\in C}^k$}.
  \item We normalize $x_c$ to be some ratio $0 \le x_c \le 1$ as follows: $X= \sum_{c\in C} x_c$ then $x_c = x_c/X$
    \ei
    If  $e$ is the ratio of classes within a software project containing errors, then $E$ 
    is the expected value  of selecting a cell {\em and} that cell containing errors:
    \begin{equation}\label{eq:E}
     E   = \sum_{c\in C}c \times x_ce
    \end{equation}
    where $c$ comes from \eq{c} and $e$ is the ratio of classes in the training set with defects.


Using these equations, we can determine how many
training examples $n$ are required before $p(E,n)$, from 
\eq{p}, returns a value more than some reasonable
threshold $T$.  To make that determination, we call
$p(E,n)$ for increasing values of $n$ until $p \ge
T$ (for this paper, we used $T = 67\%$).

For completeness, it should be added  that the procedure of the above paragraph is an {\em upper bound} on the
number of examples needed to find a signal since it
assumes random sampling of a skewed distribution. In
practice, if a data mining algorithm is smart, then
it would {\em increase} the probability of finding
the target signal, thus {\em decreasing} how many samples are required.

\subsubsection{Monte Carlo Simulation}
    The above maths lets us define
    a  Monte Carlo simulation to assess the external validity of our results.
    1000 times, we picked $k,d,b,e$ values at random from:
    \bi
      \item $k \in \{1,2,3,4,5\}$;
  \item $d \in \{3,4,5,6,7\}$ dimensions;
  \item $b \in \{2,3,4,5,6,7\}$ bins; 
    \item $e\in \{0.1,0.2,0.3,0.4\}$
      \ei
      (These ranges were set using our experience with data mining, For example, our prior work shows in defect prediction data sets
      with 40 or more dimensions, that good predictors can be built using $d\le 7$ of those dimensions~\cite{Menzies07}.)
      
     1000 times,
     we increased $n$ until \eq{p} showed  $p$ passed our reasonable threshold.
     Next, we generated examples of what $n$ value was found using  $k,b,d,e$.
     
\begin{figure}
\renewcommand{\baselinestretch}{0.75}
\begin{alltt}\scriptsize 
     1	  dimensions = (1,2)
     2	  |   dimensions = 1
     3	  |   |   e \(\le\) 0.1
     4	  |   |   |   bins = (1,2,3) : \(n < 50\) 
     5	  |   |   |   bins > 3       : \(n < 100\) 
     6	  |   |   e > 0.1 : \(n < 50\)  
     7	  |   dimensions > 1
     8	  |   |   bins = (1,2,3)
     9	  |   |   |   e = 0.1 : \(n < 100\) 
    10	  |   |   |   e > 0.1 : \(n < 50\)  
    11	  |   |   bins > 3
    12	  |   |   |   bins = (4,5)
    13	  |   |   |   |   e = 0.1 : \(n < 200\) 
    14	  |   |   |   |   e > 0.1
    15	  |   |   |   |   |   e \(\le\) 0.2
    16	  |   |   |   |   |   |   bins = 4 : \(n < 100\) 
    17	  |   |   |   |   |   |   bins = 5 : \(n < 200\) 
    18	  |   |   |   |   |   e > 0.2      : \(n < 100\) 
    19	  |   |   |   bins > 5
    20	  |   |   |   |   e \(\le\) 0.2 : \(n \ge 200 \) 
    21	  |   |   |   |   e > 0.2 : \(n < 200\)
    22	  dimensions > 2
    23	  |   bins = (1,2)
    24	  |   |   dimensions = (3,4,5)
    25	  |   |   |   dimensions = 3
    26	  |   |   |   |   e \(\le\) 0.2 : \(n < 200\) 
    27	  |   |   |   |   e > 0.2 : \(n < 50\) 
    28	  |   |   |   dimensions = (4,5)
    29	  |   |   |   |   e = 0.1 : \(n < 200\) 
    30	  |   |   |   |   e > 0.1
    31	  |   |   |   |   |   dimensions = 4 : \(n < 100\) 
    32	  |   |   |   |   |   dimensions = 5
    33	  |   |   |   |   |   |   e \(\le\) 0.3 : \(n < 200\) 
    34	  |   |   |   |   |   |   e > 0.3 : \(n < 100\) 
    35	  |   |   dimensions > 5 : \(n \ge 200 \) 
    36	  |   bins > 2
    37	  |   |   dimensions = 3
    38	  |   |   |   bins = (3,4)
    39	  |   |   |   |   e \(\le\) 0.3
    40	  |   |   |   |   |   bins = 3
    41	  |   |   |   |   |   |   e = 0.1 : \(n \ge 200 \) 
    42	  |   |   |   |   |   |   e > 0.1 : \(n < 200\) 
    43	  |   |   |   |   |   bins = 4 : \(n \ge 200 \) 
    44	  |   |   |   |   e > 0.3 : \(n < 200\) 
    45	  |   |   |   bins > 4 : \(n \ge 200 \) 
    46	  |   |   dimensions > 3 : \(n \ge 200 \)
\end{alltt}
\caption{How many $n$ examples are required to be at least 67\%
likely to find defects occuring at probability $e$.}\label{fig:dtree}
\end{figure}
\subsubsection{Decision Tree Learning}

    These examples were given to a decision tree learner to determine what $n$ values are selected by different
    ranges of $\{k,b,d,e\}$. Decision tree learners seek an attribute range that, when used to split the data,
      simplifies the distribution of the dependent variable in each split.
      The decision tree learner is then called recursively on each split.
      To test the stability of the learned model, the learning is repeated ten times, each time using 90\% of the data from training and the rest
      for testing. The weighted average performance values for the learned decision tree were remarkably good:
      \bi
    \item False alarm rates = 2\%;
    \item F-measures (i.e. the harmonic mean of recall and precision) of 95\%
      \ei
\subsection{Results}

The resulting decision tree, shown in \fig{dtree}, defined regions where
building defect predictors would be very easy and much harder:
\bi
\item
Lines 2 to 6 discuss a very easy case. Here, we only need
one dimension to build defect predictors and,
for such simple data sets, 50 to 100 examples are enough for defect prediction.
\item
Lines 22, 36, 46 show a branch of the decision tree
where needs many dimensions that divide into many bins.
For such data sets, we require a larger number of samples to learn a predictor ($n \ge 200$).
\ei
The key part of \fig{dtree} is the ``typical'' region defined in \tion{data};
i.e.    $b=2, d \le 3$:
\bi
\item Lines 7 to 10 show one set of branches covering this ``typical'' region. Note
lines 9,10:  we need up to 100 examples when the defect signal is rare (10\%) but
far fewer when the signal occurs at $e>10$\%.
\item
Lines 22 to 27 show another set of branches in this ``typical region''. Note lines 26, 27:
we need up 50 to 200 examples.
\ei
\subsection{Summary}
Our experiments with transfer learning showed that 50 to 200 examples are needed for adequate
transfer of defect knowledge. If the reader doubts that this number is too small to be effective,
we note that:
\bi
\item Other researchers working with Logistic Regression~\cite{peduzzi1996simulation}
have reported that 10 ``events per decision variable'' are adequate for building models using that
data miner. For our defect data sets, in \fig{small_epv}, we repeated the analysis of~\cite{peduzzi1996simulation}
and found that considering 2 independent variable on average, there was little improvement after 100 examples with EPV = 10 (as would have been predicted by~\cite{peduzzi1996simulation}).
\item The maths of \tion{xplain} shows that this ``100 examples are enough'' is a feature of the kinds of
data being explored; specifically, how many dimensions are needed and how many bins are required when dividing
those dimensions.
\ei
