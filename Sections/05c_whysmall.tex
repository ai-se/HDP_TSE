

\input{dims}

\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}

The above results are astonishing (to say the least): a small numher
of examples is sufficient to build a defect predictor. It turns out that, with a little sampling theory
it is possible to predict what kinds of data {\em will} and {\em will not} need so few samples in order to learn an adequate predictor.

To understand that theory, we first image we are defining a  project using three dimensions.
For this example, we will suppose that the
the {\em vertical dimension} is the number of known errors per class while
the {\em horizontal dimensions} could be any of the project descriptions seen \fig{ck}. For this example, we will say:
  \bi
  \item The east-west dimension is the lines of code per class;
  \item The north-south dimensions is the number of children per class.
    \ei
    Whatever pair of horizontal dimensions are used, we can divide those dimensions into some {\em bins}
    If we had  divided our horizontal dimensions into eight {\em bins}, the resulting space
    could be viewed as a chess board which is mostly flat but, in some squares, there
    are piles of defective classes.

    A data mining algorithm seeks some combination of the dimensions that selects for some target class (in our case,
    classes with a pile of defective classes). If we select a cell of that chess board at random, what is the probability
    we will find defective classes in that square? In the following, we will assume a ``chess board'' with $d$ dimensions
    divided into $b$ bins.

    If an event occurs at probability $p$,
    then after $n$ samples then 
    we can miss it with probability $(1-p)^n$ and find it with probability
    \begin{equation}\label{eq:pb}
      q=1-((1-p)^n)
    \end{equation}
    Next, we say that if software is described using   $d$ dimensions, each divided into $b$ bins, then the 
    project divides into   $C=b^d$ cells.
    The probability of selecting any particular cell is hence $1/C = b^{-d}$.
    
    The  errors $e$ of a project are spread around that space $s$ according to a {\em skew} variable. At $\textrm{skew}=1$, they are evenly distributed over all cells.
    At other values of $\textrm{skew}$, some parts of the space contain more errors than others, computed as follows:
    \bi
  \item $R_c$ is a random number $0\le R \le 1$, selected for each part of the space $c\in C$.
  \item $x_c$ is the proportion of errors in each part of $C$. \mbox{$x_c =  R_{c\in C}^\textrm{skew}$}.
  \item We normalize $x_c$ to be some ratio $0 \le x_c \le 1$ as follows: $X= \sum_{c\in C} x_c$ then $x_c = x_c/X$
    \ei
    If  $e$ is the ratio of classes within a software projects containing errors, then 
    the expected value of the probability $p$ of selecting a cell {\em and} that cell containing errors is:
    \begin{equation}\label{eq:p}
      p = \sum_{c\in C}b^{-d}ex_c
    \end{equation}
    The last step in this process is to use \eq{p} to find $p$, which is then passed to \eq{pn} to determine $q$;
    i.e. the probability we
    will find defects in any cell.

    Using a small Monte Carlo simulation, we can explore at what point it is likely that $n$ samples will stumble over a defect. 
    1000 times, we picked $\textrm{skew},b,d,e$ values at random from:
    \bi
      \item $\textrm{skew} \in {1,2,3,4,5}$;
  \item $d \in {3,4,5,6,7}$ dimensions;
  \item $b \in \{2,3,4,5,6,7\}$ bins; 
    \item $e\in \{0.1,0.2,0.3,0.4\}$
      \ei
      (These ranges were set using our experience with data mining, For example, our prior work shows in defect prediction data sets
      with 40 or more diminesions, that good predictors can be build using $d\le 7$ of those dimensions~\cite{me07a}.)
      
    For each of the 1000 repeats,
     we increased $n$ until \eq{pb} reported that $q$ passed some reasonable threshold (for these experiments, we used $q>67\%$).
     Next, we logged the 1000 examples of   $\textrm{skew},b,d,e,n$ found via this process.
     
    To summarize this analysis, we discretized $n$ into $n<50, n<100, n<200,n\ge200$ which we denoted {\em lt50, lt100, lt200,ge200}, respectively.
    This data was given to a decision tree learner to determine what $n$ values are selected by different
    ranges of $\{\textrm{skew},b,d,e\}$. Decision tree learners seek an attribute range that, when used to split the data,
      simplifies the distribution of the dependent variable in each split (in our case, {\em lt50, lt100, lt200,ge200}).
      The decision tree learner is then called recursively on each split.

      To test the stability of the learned model, the learning is repeated ten times, each time using 90\%of the data from training and the rest
      for testing. The weighted average performance values for the decision tree of \fig{tree} were remarkably good:
      \bi
    \item False alarm rates = 2\%;
    \item F-measures (i.e. the harmonic mean of recall and precision) of 95\%
      \ei

      
      
