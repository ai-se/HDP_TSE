
\begin{figure*}[t]
	\centering
% 	\vspace{0.5mm}
	\includegraphics[width=.66\linewidth]{Figures/raleigh/sample_random.eps}
	\caption{Improvements of using sampled data over all data with sampled size N = \{50, 100, 150, 200\}. The Right side show  using small dataset is better than using all data. We label the data in table \ref{tab:datasets} from a to z, and continue from A to H, the last two datasets ar5 and ar6 as G and H.}
	\label{fig:small_data}
\end{figure*}

\section{Size Limits (lower bounds) for Effective Transfer Learning}
\label{sec:sizelimit}

In this section, we investigate the lower bounds of the effective sizes of source and target datasets for HDP models to address RQ2.

{\bf RQ2: What are the  lower  bounds  of  the  size  of source and target  datasets  for  effective HDP?}

Since HDP compares the distributions of source metrics to those of target metrics, it is important to seek the empirical evidence for the effective sizes of source and target datasets to match source and target metrics. We first present the results of the empirical study for RQ2 in this section and validate the generality of its results in Section~\ref{sect:xplain}.

Like prior work \cite{Nam13,
  Ma12, Rahman12, Ryu14,
  Zhang14}, the basic HDP method we
proposed above uses {\em all} the instances in potential source and target projects to perform metric to select the best matched metrics and then build
defect prediction learners.
Collecting {\em all} that data from source and target projects need much more work and also
for the target project, it requires waiting for it to finish before
transferring its learned lessons. This begs the question ``how early can we transfer?''.
That is, how {\em few} historical data and target projects do we need before transfer can be effective? In this section, we conduct an empirical study to answer these questions related to RQ2.

To investigate the size limits for effective transfer learning in the setting of CPDP across datasets with heterogeneous metric sets, we focus on the HDP approach. There are other approaches such as CPDP-IFS~\cite{He14} and CCA+~\cite{Jing15}. In Section~\ref{sec:Result}, we observed that HDP outperforms CPDP-IFS. In addition, CCA+ was evaluated in somewhat different context, i.e., cross-company defect prediction and with 14 projects which are far less than 34 projects used in our experiments for HDP. In addition, the implementation of CCA+ is not publicly available yet and more complex than HDP. For this reason, we conducted our empirical study for RQ2 based on HDP.



 
\subsection{Using Small Datasets is Feasible}

Recall from the above,
HDP uses  datasets  in a two step process.
To test the impact of having access to {\em less} data,
we  add an instance sampling process before performing metric matching:
instead of using all the instances from
candidate source and target datasets, those datasets will
be randomly sampled (without replacement) to generate smaller datasets of
size $N \in \{50, 100, 150, 200\}$. 

The reason we choose those $N$ values as follows. On one hand, by looking at the size of datasets used in the
above experiment, we observed that minimum size is ar5 with $36$ instances and maximum size is pc5 with $17001$ instances.
The median and mean value of dataset size are $1505$ and $325$, respectively. Then $200$ is less than both of 
them, which is reasonably small.
On the other hand, we use these numbers to show using small datasets is feasible compared to
the original. We're not claiming they're the best~(optimal) small numbers. For most datasets considered in this experiment, $N$ with these values is a 
small data size compared to the original data size. 

When sampling the original data, {\it if
the number of instances in the original dataset is
smaller than $N$, all those instances will be
included}. For example, $N=200$ means we sampled both source and target data with size of $200$. If the dataset has less than $200$ instances, such as ar3, we use all
the instances and no oversampling is applied. With those sampled $N=200$ data, we perform metric matching to build a learner
and finally predict labels of all original data in the target project. We sample the data without replacement
to void duplicate data.

The results for this HDP-with-limited-data experiment is shown in \fig{small_data}
(we display median AUC results from 20 repeats, using  Logistic Regression
as the default learner). 
% \lin{It's unclear to me. So metric matching on the 50, 100, etc.? What is third initial sampling? Add a backward reference to it? One caveat: I focused on reading the abstract, Section 3, 7, and 8, and I only skimmed over the rest of the paper, so maybe I missed it. But in general it is still good to add a backward reference.}\wei{I didn't get the question, "what is third initial sampling?", which third?}
% Since {\it Apahce commons math} library takes quite long time to conduct KS-test as mentioned, we replace it with Scipy, a python package, to speed up our experiment. \footnote{As shown in the table, the difference between those two KS-test results is acceptable}.
In that figure:
\squishlist
\item
  The {\em black} line show the results using {\em all} data;
\item
  The {\em colourful} lines show results of transferring from {\em some} small $N$ number of samples (instead of {\em all})
  in the source and target datasets during metric matching and learner building;
\item
  The letters show the unique ID of each dataset.
\squishend
\begin{figure*}[t]
	\centering
% 	\vspace{0.5mm}
	\includegraphics[width=0.66\linewidth]{Figures/raleigh/sample_epv.eps}
	\caption{Improvements of using sampled data N  = \{50, 100, 150, 200\} with EPV constraints. The Right side show  using small dataset with EPV is better than using all data. We label the data in table \ref{tab:datasets} from a to z, and continue from A to H, the last two datasets ar5 and ar6 as G and H.}
	\label{fig:small_epv}
\end{figure*}
The datasets are ordered left to right by the difference to the black line (where we transfer using {\em
  all} the source data):
\squishlist
\item
  On the left, the black line is
  {\em above} the red line; i.e., for those datasets, we do {\em better} using
  {\em all} data than using {\em some}.
  \item
On the right, the black line is {\em below} the red
line; i.e., for those datasets, we do {\em worse}
using {\em all} data than using {\em some}.
\squishend
Note that the gap between the red and black line
shrinks as we use more data and after $N=100$, the
net gap space is almost zero.  When $N=200$, in $28/34$
datasets,
it is hard to distinguish
the blue and black curves of \fig{small_data}.
That is, we conclude
that using more
than a small sample size
(like $N=200$) would
not improve defect prediction.






\subsection{20 Defective Examples  are Enough}


The results of the last section are very encouraging---a small number of source
and target examples are enough for effective transfer learning. Naturally,
we were suspicious of this result (since it was ``almost too good to be true'').
Accordingly, we explored the literature and found that:
\squishlist
    \item Evidence that this ``a few examples are enough'' has been seen in other domains~\cite{peduzzi1996simulation}; 
    \item Further, we found new methods to reduce the cost of sampling this dataset, even further. 
\squishend
Working in the domain cardiology,
  Peduzzi et al.~\cite{peduzzi1996simulation}
  report 10 ``events'' per variable (EPV) are enough to maximize the predictive performance
  of  Logistic Regression (the learner used in this study). Translated into our terminology,
  that study would predict that  10 defective data per independent variable should
  suffice for effective learning. Of course, learners require defective and {\em non-defective}
  examples so to $M$ defective examples, we add $(N-M)$ non-defective examples more.

  To apply the method of Peduzzi et al., we first instrumented HDP to determine how many
  independent variables were picked during {\em metric  selection}. In our previous experiments when using all source and target data to do feature/metric matching, we observed that the counts of different matched features are as follows: $\{1:120, 2:249, 3:171, 4:60, 5:14\}$, where keys and values represent number of matched features, the corresponding counts of cross prediction combinations, respectively. Our read of this distribution is that the median of matched features is $2$. That means usually those 2 features from source and target projects can be used to build the learner. Hence, applying  Peduzzi et al.'s rule (EPV = $10$,  in our case, \# of events = $10$, \# of variables = $2$), we picked $2\times 10=20$ %\lin{why *10?}\wei{EPV=10, mentioned in the last paragraph. }
  randomly selected defective data, then randomly added $(N-M)$ non-defective data for
  $N\in \{50,100,150,200\}$ for the source data. For the target dataset, since we do not
  know the data labels, we simply sampled $N$ for target data in metric matching.

  (Aside: note that this is different to the above experiment since, before, the more $N$ examples
  we selected, the more {\em defective} instances we would use. Now, in this experiment, we will also
  use a fixed $M=20$ number of defective files and classes.).

  The results are shown in \fig{small_epv}. Like before:
\squishlist
  \item These are median AUC results from 20 repeats;
\item
  The {\em black/colourful} lines show the results using {\em all/some} of  data from
  the source and target datasets during metric matching and learner building, respectively
  (but now, the {\em some} never contains more than $M=20$ defective instances);
\item
  The datasets are ordered left to right according
  the performance difference between using
   {\em all} or {\em some} of the data;
   \item
     On the left/right ,  we do {\em better/worse} using
  {\em all} data than using {\em some}.
\squishend
  We observe that between $N=50$ and $N=200$, the performance delta
  does not change by much. Also, if we compare $N=200$ between \fig{small_data}
  and \fig{small_epv}, there is no large disadvantage of using just
  $M=20$ defective instances.

  Note that such a very small sample would be quick to collect: since training data is already available even before we start our target (new) project, we can easily sample $N$ data with $M=20$ defective classes/files for each candidate training data. After writing (say) a few
  hundred classes for the target project, it's also easy to randomly sample $N$ from target project---at which point that data is a candidate for transfer learning.
