\section{Conclusion}
\label{sec:Conclusion}
%\sung{Overall the conclusion is weak and does not convey the excitement of your
% wonderful work + results. Think about it and rewrite it.}

In the past, cross-project defect prediction cannot be conducted across projects
with heterogeneous metric sets. To address
this limitation, we proposed heterogeneous defect prediction (HDP) based on
metric matching using statistical analysis~\cite{Massey51}. Our experiments
showed that the proposed HDP models are feasible and yield promising results. In addition, we investigated the lower bounds of the size of source and target datasets for effective transfer learning in defect prediction. Based on our empirical study, we suggested that the sample size of 200 for source (with at least 20 defective samples) and target datasets could be effective enough for our HDP models.
% The result (0.725) of the cross-domain prediction
% model by KSAnlyzer with the cutoff of 0.05 outperformed that of
% within-prediction (0.657) and cross-prediction using common features (0.645) in
% terms of a median AUC with statistical significance.

% To the best of our knowledge, this is
% the first study to focus on issues on different feature spaces for cross-project
% defect prediction.

% In our empirical study, we observed cross-domain prediction is feasible and
% promising. The result (0.725) of the cross-domain prediction
% model by KSAnlyzer with the cutoff of 0.05 outperformed that of
% within-prediction (0.657) and cross-prediction using common features (0.645) in
% terms of a median AUC with statistical significance. In addition, 65.6\% and 67.6\% of prediction
% combinations by our approach have outperforming (Win) and comparable (Tie)
% results to within-prediction and cross-prediction using common features with
% statistical significance. The Loss results were only less than about 35\%.
% However, in
% non-defect$\Rightarrow$defect, prediction coverage was about 11\% (SE) and 36\%
% (non-SE). This poor coverage shows our framework can match related features in
% defect$\Rightarrow$defect predictions.

% Say more about the implications of your work. What we can do using this? What
% new research directions you will open. (Hint: Potentially all heterogeneous
% data from other projects (such as data in PROMISE) can be used for my new
% project. Of cause, it is not limited to only defect prediction, right?)

% However, we observed some prediction combinations by our approach were still
% worse than baselines because of noisy target features that do not follow the
% bug-prone tendency of a source feature. Identifying noisy target features is
% still a challenging issue.
% This remains as future work.

HDP is very promising as it permits potentially all heterogeneous datasets
of software projects to be used for defect prediction on new projects or
projects lacking in defect data.
In addition, it may not be limited to defect prediction. This technique can
potentially be applicable to all prediction and recommendation based approaches
for software engineering problems.
As future work, for the metric
matching, we will apply other techniques, like deep learning, to explore new features from source and target project to improve the performance. Since transfer learning has shown such great power, we will explore the feasibility of building various prediction and recommendation models to solve other software engineering problems. 


% We investigated the distribution of
% matched features on Win and Loss prediction results and observed that
% the distribution similarity of matched features is important to conduct
% cross-domain defect prediction. Loss prediction results are caused by



% In addition, we measure prediction performance in terms
% of AUC. Although AUC is a useful measure to compare different prediction
% models~\cite{Giger12,Lessmann08,Rahman12,Song11}, precision and recall should
% also be considered to use our cross-domain defect prediction models in practice.
% However, to fairly compare prediction models in terms of precision and recall,
% we should identify a proper threshold of prediction probability.
% This is another challenging issue, which we keep as future
% work.
